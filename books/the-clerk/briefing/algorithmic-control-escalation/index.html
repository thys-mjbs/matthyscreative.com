<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />

  <title>Matthys Creative</title>
  <meta name="description" content="Analyses, briefings, and scenario-based narrative entry points." />

  <link rel="icon" href="/01-assets/style/favicon.ico" type="image/x-icon" />

  <!-- Global CSS -->
  <link rel="stylesheet" href="/01-assets/style/global.css" />
</head>

<body>

  <!-- HEADER -->
  <header class="site-header">
    <div class="site-header-inner">

      <div class="site-brand">
        <a href="/">Matthys Creative</a>
      </div>

      <nav class="site-nav">
        <a href="/home/">Home</a>
        <a href="/about/">About</a>
        <a href="/contact/">Contact</a>
      </nav>

    </div>
  </header>

  <!-- MAIN CONTENT -->
  <main class="site-main">

    <section>
      <h1>The Myth That Algorithms Remove Bias</h1>
      <p>The dominant belief is that algorithmic systems reduce bias by replacing human judgment. In practice, they relocate bias into structures that are harder to see, harder to challenge, and more durable over time.</p>
    
      <h2>Why Consistency Is Mistaken for Fairness</h2>
      <p>Algorithms deliver consistent outputs. This consistency is interpreted as neutrality. Organizations equate repeatability with objectivity, even when the underlying criteria are arbitrary or outdated.</p>
      <p>Once deployed, consistency becomes a shield. Disputes are deflected with references to standard procedure. The question shifts from “Was this correct?” to “Was this processed correctly?”</p>
    
      <h2>How Human Bias Is Encoded, Not Eliminated</h2>
      <p>Every model begins with human choices: which variables matter, how they are weighted, and what outcomes are prioritized. These decisions reflect institutional incentives, not moral neutrality.</p>
      <p>After deployment, these choices are no longer debated. They are operationalized. Bias becomes structural rather than personal, which makes it less contestable.</p>
    
      <h2>Why Appeals Rarely Change Outcomes</h2>
      <p>Appeals are processed through the same system that generated the decision. New information is forced into existing categories. If it does not map cleanly, it is discounted.</p>
      <p>High appeal volumes are treated as noise. The response is automation of the appeal itself, further reducing human discretion.</p>
    
      <h2>The Practical Reason Institutions Defend the System</h2>
      <p>Removing or revising an algorithm implies prior decisions may have been flawed. This introduces legal exposure and operational uncertainty.</p>
      <p>It is safer to adjust messaging than mechanics. Institutions defend the system not because it is optimal, but because it stabilizes liability.</p>
    
      <h2>What Actually Improves Outcomes</h2>
      <p>Improvement does not come from more data alone. It comes from restoring limited human authority at defined intervention points.</p>
      <p>Systems that allow structured overrides with feedback loops outperform rigid models over time. Few institutions implement this because it slows throughput.</p>
    </section>

    <section>
      <h3>Next Step</h3>
      <p>A fictional scenario illustrates how this belief collapses under real pressure:</p>
      <p><a href="https://amzn.to/3YJVREn">Explore the scenario</a></p>
    </section>
    
    <section>
      <h3>Email Capture Placeholder</h3>
      <p>Optional: receive the first chapter by email.</p>
    </section>
    
  </main>

  <!-- FOOTER -->
  <footer class="site-footer">
    <div class="site-footer-inner">

      <span>© 2026 Matthys Creative</span>

      <nav class="site-footer-nav">
        <a href="/privacy/">Privacy</a>
        <a href="/terms/">Terms</a>
        <a href="/disclaimer/">Disclaimer</a>
      </nav>

    </div>
  </footer>

</body>
</html>
